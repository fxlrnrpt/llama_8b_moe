{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7346cbd",
   "metadata": {},
   "source": [
    "# Cosine ML take-home task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2a4bc4",
   "metadata": {},
   "source": [
    "## Task Description\n",
    "\n",
    "Your task is to take Llama 3.1 8B and rearchitect the dense layers to MoE layers. You are supplied with a Llama 3 architecture below.\n",
    "\n",
    "Architect the MoE-ified Llama 8B such that it performs identically to the base model. That is, when inferencing upon a single prompt at temperature 0 the output of the base dense model and your MoE Llama should be the same. Write a test that demonstrates that the MoE-ified Llama produces the same logits as the base model to within a reasonable tolerance, such that greedy decoding produces the same token sequence in either setting. Describe and/or demonstrate the approach you took to ensure this and whether or not this differs from how you would expect to inference with the model in a production setting.\n",
    "\n",
    "When writing the MoE layer you should consider issues regarding performance and memory when inferencing in real-world scenarios. Discuss the implications on performance when optimising for memory when inferencing your MoE-ified Llama, and vice-versa. The task is open ended in the sense that you may wish to produce a more performant MoE implementation, and another implementation better suited to situations where memory is the bottleneck. However, this is not required and a detailed discussion can also prove sufficient.\n",
    "\n",
    "## Core Requirements\n",
    "\n",
    "1. Replace the dense feed-forward layers (MLP blocks) in Llama with an MoE equivalent. Your MoE design should support:\n",
    "\n",
    "- an “identity / toy mode” that guarantees logit fidelity with the base dense model\n",
    "- a “production-like mode” that resembles a realistic MoE setup (e.g., learned router + top-k routing), even if it is not trained. You may decide what “production-like” means, but it should meaningfully reflect real MoE usage (routing, multiple experts, dispatch pattern, etc.) rather than only being a no-op wrapper.\n",
    "\n",
    "2. Write a runnable test that demonstrates logit fidelity between the original dense Llama 3.1 8B model, and your MoE-ified model in identity/toy mode. The test must:\n",
    "\n",
    "- load pretrained dense weights\n",
    "- compare logits between models and assert they match within a reasonable tolerance (you decide what’s reasonable and justify it)\n",
    "- ideally include a greedy decode check to confirm identical token outputs\n",
    "\n",
    "3. A discussion regarding your MoE implementation and the performance/memory trade-offs you have considered. Consider the implications of optimizing for: \n",
    "\n",
    "- maximum throughput (fast inference)\n",
    "- minimum memory footprint \n",
    "\n",
    "As well as runnability, you will be judged on code quality and readability. With regards to the asset you produce, there are no strict requirements - you can return a notebook, or split it out into a package along with a script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814a020a",
   "metadata": {},
   "source": [
    "## Model Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e688f3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    vocab_size: int = 128256\n",
    "    hidden_size: int = 4096\n",
    "    intermediate_size: int = 14336\n",
    "    num_hidden_layers: int = 32\n",
    "    num_attention_heads: int = 32\n",
    "    num_key_value_heads: int = 8  # GQA\n",
    "    max_seq_len: int = 8192\n",
    "    rms_norm_eps: float = 1e-5\n",
    "    rope_theta: float = 500000.0\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        variance = x.pow(2).mean(-1, keepdim=True)\n",
    "        x = x * torch.rsqrt(variance + self.eps)\n",
    "        return self.weight * x\n",
    "\n",
    "\n",
    "def rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
    "    x1, x2 = x[..., : x.shape[-1] // 2], x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_position_emb(q, k, cos, sin):\n",
    "    # q, k: [bs, nheads, seq, head_dim]\n",
    "    cos = cos.unsqueeze(0).unsqueeze(0)\n",
    "    sin = sin.unsqueeze(0).unsqueeze(0)\n",
    "    q = (q * cos) + (rotate_half(q) * sin)\n",
    "    k = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q, k\n",
    "\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim: int, max_seq_len: int, base: float = 10000.0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.max_seq_len_cache = max_seq_len\n",
    "        self.base = base\n",
    "\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        self._set_cos_sin_cache(max_seq_len)\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len: int):\n",
    "        t = torch.arange(seq_len, device=self.inv_freq.device).float()\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        cos = emb.cos()\n",
    "        sin = emb.sin()\n",
    "        self.register_buffer(\"cos_cached\", cos, persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", sin, persistent=False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, seq_len: int):\n",
    "        if seq_len > self.cos_cached.shape[0]:\n",
    "            self._set_cos_sin_cache(seq_len)\n",
    "        return self.cos_cached[:seq_len], self.sin_cached[:seq_len]\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.num_kv_heads = config.num_key_value_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "\n",
    "        assert self.head_dim * self.num_heads == self.hidden_size, (\n",
    "            \"hidden_size must be divisible by num_heads\"\n",
    "        )\n",
    "        assert self.num_heads % self.num_kv_heads == 0, (\n",
    "            \"num_heads must be multiple of num_kv_heads\"\n",
    "        )\n",
    "\n",
    "        self.q_proj = nn.Linear(\n",
    "            self.hidden_size, self.num_heads * self.head_dim, bias=False\n",
    "        )\n",
    "        self.k_proj = nn.Linear(\n",
    "            self.hidden_size, self.num_kv_heads * self.head_dim, bias=False\n",
    "        )\n",
    "        self.v_proj = nn.Linear(\n",
    "            self.hidden_size, self.num_kv_heads * self.head_dim, bias=False\n",
    "        )\n",
    "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
    "\n",
    "        self.rotary_emb = RotaryEmbedding(\n",
    "            dim=self.head_dim,\n",
    "            max_seq_len=config.max_seq_len,\n",
    "            base=config.rope_theta,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "        if n_rep == 1:\n",
    "            return x\n",
    "        bsz, kv_heads, seq, hd = x.shape\n",
    "        x = x[:, :, None, :, :].expand(bsz, kv_heads, n_rep, seq, hd)\n",
    "        return x.reshape(bsz, kv_heads * n_rep, seq, hd)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: [bsz, seq, hidden]\n",
    "        past_key_value:\n",
    "          k: [bsz, kv_heads, past_seq, head_dim]\n",
    "          v: [bsz, kv_heads, past_seq, head_dim]\n",
    "        \"\"\"\n",
    "        bsz, seq_len, _ = x.shape\n",
    "\n",
    "        q = (\n",
    "            self.q_proj(x)\n",
    "            .view(bsz, seq_len, self.num_heads, self.head_dim)\n",
    "            .transpose(1, 2)\n",
    "        )  # [bsz, heads, seq, hd]\n",
    "        k = (\n",
    "            self.k_proj(x)\n",
    "            .view(bsz, seq_len, self.num_kv_heads, self.head_dim)\n",
    "            .transpose(1, 2)\n",
    "        )  # [bsz, kv, seq, hd]\n",
    "        v = (\n",
    "            self.v_proj(x)\n",
    "            .view(bsz, seq_len, self.num_kv_heads, self.head_dim)\n",
    "            .transpose(1, 2)\n",
    "        )\n",
    "\n",
    "        cos, sin = self.rotary_emb(seq_len)\n",
    "        q, k = apply_position_emb(q, k, cos, sin)\n",
    "\n",
    "        n_rep = self.num_heads // self.num_kv_heads\n",
    "        k = self.repeat_kv(k, n_rep)  # [bsz, heads, total_seq, hd]\n",
    "        v = self.repeat_kv(v, n_rep)\n",
    "\n",
    "        total_seq = k.size(2)\n",
    "\n",
    "        attn_scores = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(\n",
    "            self.head_dim\n",
    "        )  # [bsz, heads, seq, total_seq]\n",
    "        causal = torch.tril(\n",
    "            torch.ones(seq_len, total_seq, device=x.device, dtype=torch.bool)\n",
    "        )\n",
    "        attn_scores = attn_scores.masked_fill(~causal, float(\"-inf\"))\n",
    "\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        out = torch.matmul(attn_probs, v)  # [bsz, heads, seq, hd]\n",
    "        out = out.transpose(1, 2).contiguous().view(bsz, seq_len, self.hidden_size)\n",
    "        out = self.o_proj(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.down_proj(F.silu(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.attn_norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.self_attn = Attention(config)\n",
    "        self.ffn = FeedForward(config)\n",
    "        self.ffn_norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        residual = x\n",
    "        attn_out = self.self_attn(self.attn_norm(x))\n",
    "        x = residual + attn_out\n",
    "\n",
    "        residual = x\n",
    "        x = residual + self.ffn(self.ffn_norm(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [DenseBlock(config) for _ in range(config.num_hidden_layers)]\n",
    "        )\n",
    "        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.embed_tokens(x)  # [bsz, seq, hidden]\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.norm(x)\n",
    "        return self.lm_head(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b315a04",
   "metadata": {},
   "source": [
    "## State Dict loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72505c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from safetensors.torch import load_file as safetensors_load_file\n",
    "\n",
    "mapping = {\n",
    "    \".mlp\": \".ffn\",\n",
    "    \".post_attention_layernorm\": \".ffn_norm\",\n",
    "    \".input_layernorm\": \".attn_norm\",\n",
    "}\n",
    "\n",
    "\n",
    "def load_weights(\n",
    "    model: torch.nn.Module,\n",
    "    model_dir: str | None,\n",
    "    dtype: torch.dtype | None = None,\n",
    "    device: torch.device | None = None,\n",
    "    strict: bool = False,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Minimal loader for a model whose weights are sharded across multiple safetensors files.\n",
    "\n",
    "    Maps between the llama weight names as given in the HF safetensors and the parameter names in the above implementation.\n",
    "    \"\"\"\n",
    "\n",
    "    sd = {}\n",
    "    if model_dir is not None:\n",
    "        # Read model.safetensors.index.json\n",
    "        index_path = os.path.join(model_dir, \"model.safetensors.index.json\")\n",
    "        if not os.path.isfile(index_path):\n",
    "            raise FileNotFoundError(f\"Missing index file: {index_path}\")\n",
    "\n",
    "        with open(index_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            idx = json.load(f)\n",
    "\n",
    "        if \"weight_map\" not in idx:\n",
    "            raise ValueError(f\"Index file missing 'weight_map': {index_path}\")\n",
    "\n",
    "        weight_map = idx[\"weight_map\"]\n",
    "        shard_files = sorted(set(weight_map.values()))\n",
    "\n",
    "        # Load each sharded list in one merged state_dict\n",
    "        for shard in shard_files:\n",
    "            shard_path = os.path.join(model_dir, shard)\n",
    "            if not os.path.isfile(shard_path):\n",
    "                raise FileNotFoundError(\n",
    "                    f\"Shard referenced by index not found: {shard_path}\"\n",
    "                )\n",
    "            sd.update(safetensors_load_file(shard_path))\n",
    "    else:\n",
    "        shard_files = []\n",
    "\n",
    "    # iterate over keys and rename according to mapping\n",
    "    # optionally cast dtype and/or move to device\n",
    "    for param in list(sd.keys()):\n",
    "        weights = sd.pop(param)\n",
    "        if param.startswith(\"model.\"):\n",
    "            param = param.replace(\"model.\", \"\")\n",
    "        for old, new in mapping.items():\n",
    "            if old in param:\n",
    "                param = param.replace(old, new)\n",
    "        if dtype is not None:\n",
    "            weights = weights.to(dtype)\n",
    "        if device is not None:\n",
    "            weights = weights.to(device)\n",
    "        sd[param] = weights\n",
    "\n",
    "    meta = {\n",
    "        \"model_dir\": model_dir,\n",
    "        \"num_shards\": len(shard_files),\n",
    "        \"shards\": shard_files,\n",
    "    }\n",
    "\n",
    "    if not sd:\n",
    "        for name, tensor in model.state_dict().items():\n",
    "            if tensor.is_floating_point():\n",
    "                rand = torch.randn_like(tensor)\n",
    "            else:\n",
    "                rand = torch.zeros_like(tensor)\n",
    "\n",
    "            if dtype is not None and rand.is_floating_point():\n",
    "                rand = rand.to(dtype)\n",
    "            if device is not None:\n",
    "                rand = rand.to(device)\n",
    "\n",
    "            sd[name] = rand\n",
    "\n",
    "    # Load state_dict into model\n",
    "    missing, unexpected = model.load_state_dict(sd, strict=strict)\n",
    "    model.to(device=device)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"=== load_weights_sharded_safetensors_only ===\")\n",
    "        print(f\"model_dir: {model_dir}\")\n",
    "        print(f\"num_shards: {meta['num_shards']}\")\n",
    "        print(f\"num_tensors_loaded: {len(sd)}\")\n",
    "        print(f\"missing_keys: {len(missing)}\")\n",
    "        print(f\"unexpected_keys: {len(unexpected)}\")\n",
    "\n",
    "    return {\n",
    "        \"model_dir\": model_dir,\n",
    "        \"checkpoint_meta\": meta,\n",
    "        \"num_tensors_loaded\": len(sd),\n",
    "        \"missing_keys\": missing,\n",
    "        \"unexpected_keys\": unexpected,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77074d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = f\"{os.path.expanduser('~')}/.cache/huggingface/hub/\"\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "snapshots_dir = os.path.join(\n",
    "    cache_dir, \"models--\" + model_name.replace(\"/\", \"--\"), \"snapshots\"\n",
    ")\n",
    "assert os.path.isdir(snapshots_dir), f\"Model directory not found: {snapshots_dir}\"\n",
    "model_dir = os.path.join(snapshots_dir, os.listdir(snapshots_dir)[0])\n",
    "\n",
    "config = ModelConfig()\n",
    "model = Transformer(config)\n",
    "\n",
    "load_weights(\n",
    "    model,\n",
    "    model_dir=model_dir,\n",
    "    dtype=torch.bfloat16,\n",
    "    device=\"cuda\",\n",
    "    strict=True,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc69706",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, PreTrainedTokenizer\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a helpful coding assistant. Answer the user's questions like a pirate.\"\n",
    ")\n",
    "user_prompt = \"Explain the difference between a list and a tuple in Python.\"\n",
    "\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n",
    "\n",
    "tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interview-tasks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
